{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0516a83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2 \n",
    "from cv2imshow.cv2imshow import cv2_imshow_single\n",
    "IMAGE_FILE = 'maxresdefault.jpg'\n",
    "\n",
    "img = cv2.imread(IMAGE_FILE)\n",
    "window_name = 'Image'\n",
    "cv2.imshow(window_name, img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4638e704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in f:\\coding\\.venv\\lib\\site-packages (0.10.21)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: absl-py in f:\\coding\\.venv\\lib\\site-packages (from mediapipe) (2.3.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in f:\\coding\\.venv\\lib\\site-packages (from mediapipe) (25.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in f:\\coding\\.venv\\lib\\site-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in f:\\coding\\.venv\\lib\\site-packages (from mediapipe) (0.6.2)\n",
      "Requirement already satisfied: jaxlib in f:\\coding\\.venv\\lib\\site-packages (from mediapipe) (0.6.2)\n",
      "Requirement already satisfied: matplotlib in f:\\coding\\.venv\\lib\\site-packages (from mediapipe) (3.10.6)\n",
      "Requirement already satisfied: numpy<2 in f:\\coding\\.venv\\lib\\site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in f:\\coding\\.venv\\lib\\site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in f:\\coding\\.venv\\lib\\site-packages (from mediapipe) (4.25.8)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in f:\\coding\\.venv\\lib\\site-packages (from mediapipe) (0.5.2)\n",
      "Requirement already satisfied: sentencepiece in f:\\coding\\.venv\\lib\\site-packages (from mediapipe) (0.2.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in f:\\coding\\.venv\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
      "Requirement already satisfied: pycparser in f:\\coding\\.venv\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in f:\\coding\\.venv\\lib\\site-packages (from jax->mediapipe) (0.5.3)\n",
      "Requirement already satisfied: opt_einsum in f:\\coding\\.venv\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.12 in f:\\coding\\.venv\\lib\\site-packages (from jax->mediapipe) (1.15.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in f:\\coding\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in f:\\coding\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in f:\\coding\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in f:\\coding\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in f:\\coding\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in f:\\coding\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in f:\\coding\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in f:\\coding\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in f:\\coding\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "pip install mediapipe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ee357b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 1: Import the necessary modules.\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "def visualize(image: np.ndarray, detection_result) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Draw bounding boxes and labels from MediaPipe ObjectDetector on an image.\n",
    "    \"\"\"\n",
    "    for detection in detection_result.detections:\n",
    "        # Bounding box\n",
    "        bbox = detection.bounding_box\n",
    "        start_point = (int(bbox.origin_x), int(bbox.origin_y))\n",
    "        end_point = (int(bbox.origin_x + bbox.width), int(bbox.origin_y + bbox.height))\n",
    "        cv2.rectangle(image, start_point, end_point, (0, 255, 0), 2)\n",
    "\n",
    "        # Label and score\n",
    "        if detection.categories:\n",
    "            category = detection.categories[0]\n",
    "            label = f\"{category.category_name}: {category.score:.2f}\"\n",
    "            cv2.putText(image, label, (start_point[0], start_point[1]-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)\n",
    "    return image\n",
    "\n",
    "# Download the model if it does not exist\n",
    "model_path = 'efficientdet.tflite'\n",
    "if not os.path.exists(model_path):\n",
    "    url = 'https://storage.googleapis.com/mediapipe-models/object_detector/efficientdet_lite0/int8/1/efficientdet_lite0.tflite'\n",
    "    urllib.request.urlretrieve(url, model_path)\n",
    "\n",
    "# STEP 2: Create an ObjectDetector object.\n",
    "base_options = python.BaseOptions(model_asset_path=model_path)\n",
    "options = vision.ObjectDetectorOptions(base_options=base_options,\n",
    "                                       score_threshold=0.5)\n",
    "detector = vision.ObjectDetector.create_from_options(options)\n",
    "\n",
    "# STEP 3: Load the input image.\n",
    "image = mp.Image.create_from_file(IMAGE_FILE)\n",
    "\n",
    "# STEP 4: Detect objects in the input image.\n",
    "detection_result = detector.detect(image)\n",
    "\n",
    "# STEP 5: Process the detection result. In this case, visualize it.\n",
    "image_copy = np.copy(image.numpy_view())\n",
    "annotated_image = visualize(image_copy, detection_result)\n",
    "rgb_annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
    "window_name = 'Image'\n",
    "cv2.imshow(window_name, rgb_annotated_image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3db1bb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc9f8c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model to f:\\coding\\.venv\\lib\\site-packages\\mediapipe/modules/pose_landmark/pose_landmark_heavy.tflite\n"
     ]
    }
   ],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Create Pose object\n",
    "pose = mp_pose.Pose(static_image_mode=True,       # True for image, False for video stream\n",
    "                    model_complexity=2,          # 0,1,2 â†’ higher is more accurate\n",
    "                    enable_segmentation=False,\n",
    "                    min_detection_confidence=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc9246ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"maxresdefault.jpg\")\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # MediaPipe expects RGB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65a9b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pose.process(image_rgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f9402d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results.pose_landmarks:\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image, \n",
    "        results.pose_landmarks, \n",
    "        mp_pose.POSE_CONNECTIONS,     # Draw connections between joints\n",
    "        landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0,255,0), thickness=2, circle_radius=2),\n",
    "        connection_drawing_spec=mp_drawing.DrawingSpec(color=(0,0,255), thickness=2)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ed27919",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Pose Landmarks\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f8e8d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated video saved to output_pose2.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Pose and Drawing utilities\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "pose = mp_pose.Pose(static_image_mode=False,\n",
    "                    model_complexity=2,\n",
    "                    enable_segmentation=False,\n",
    "                    min_detection_confidence=0.5,\n",
    "                    min_tracking_confidence=0.5)\n",
    "\n",
    "# Input and output video paths\n",
    "input_video_path = \"male_bad_discus.mov\"\n",
    "output_video_path = \"output_pose2.mp4\"\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(f\"Cannot open video {input_video_path}\")\n",
    "\n",
    "# Get video properties\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Video writer to save annotated video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "reference_landmarks = []  # list of frames\n",
    "user_landmarks = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert BGR to RGB for MediaPipe\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process frame\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    # Draw landmarks if detected\n",
    "    if results.pose_landmarks:\n",
    "        frame_landmarks = []\n",
    "        for lm in results.pose_landmarks.landmark:\n",
    "            frame_landmarks.append((lm.x, lm.y, lm.z))  # normalized coords\n",
    "        reference_landmarks.append(frame_landmarks)\n",
    "        \n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0,255,0), thickness=2, circle_radius=2),\n",
    "            connection_drawing_spec=mp_drawing.DrawingSpec(color=(0,0,255), thickness=2)\n",
    "        )\n",
    "\n",
    "    # Write annotated frame to output\n",
    "    out.write(frame)\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "pose.close()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Annotated video saved to {output_video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d02d576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def angle_between_points(a, b, c):\n",
    "    \"\"\"\n",
    "    Computes angle at point b (in degrees) formed by points a-b-c\n",
    "    \"\"\"\n",
    "    ba = np.array(a) - np.array(b)\n",
    "    bc = np.array(c) - np.array(b)\n",
    "    cos_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.degrees(np.arccos(np.clip(cos_angle, -1.0, 1.0)))\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08eea314",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reference_angles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m differences \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ref_frame, user_frame \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mreference_angles\u001b[49m, user_angles):\n\u001b[0;32m      3\u001b[0m     frame_diff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(np\u001b[38;5;241m.\u001b[39marray(ref_frame) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39marray(user_frame))\n\u001b[0;32m      4\u001b[0m     differences\u001b[38;5;241m.\u001b[39mappend(frame_diff)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'reference_angles' is not defined"
     ]
    }
   ],
   "source": [
    "differences = []\n",
    "for ref_frame, user_frame in zip(reference_angles, user_angles):\n",
    "    frame_diff = np.abs(np.array(ref_frame) - np.array(user_frame))\n",
    "    differences.append(frame_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "783c139e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing reference video...\n",
      "Processing user video...\n",
      "Creating comparison video...\n",
      "Comparison video saved to comparison_overlay.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Helper functions\n",
    "# -----------------------------\n",
    "def extract_landmarks(video_path):\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(static_image_mode=False,\n",
    "                        model_complexity=2,\n",
    "                        enable_segmentation=False,\n",
    "                        min_detection_confidence=0.5,\n",
    "                        min_tracking_confidence=0.5)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    landmarks_seq = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(image_rgb)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            landmarks_seq.append(results.pose_landmarks)\n",
    "        else:\n",
    "            landmarks_seq.append(None)  # placeholder\n",
    "\n",
    "    cap.release()\n",
    "    pose.close()\n",
    "    return frames, landmarks_seq\n",
    "\n",
    "def draw_landmarks_on_frame(frame, landmarks, mp_drawing, mp_pose):\n",
    "    if landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame,\n",
    "            landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0,255,0), thickness=2, circle_radius=2),\n",
    "            connection_drawing_spec=mp_drawing.DrawingSpec(color=(0,0,255), thickness=2)\n",
    "        )\n",
    "    return frame\n",
    "\n",
    "# -----------------------------\n",
    "# Main script\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    reference_video = \"Women_final_discus.mp4\"\n",
    "    user_video = \"Male_final_discus.mp4\"\n",
    "    output_video_path = \"comparison_overlay.mp4\"\n",
    "\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_pose = mp.solutions.pose\n",
    "\n",
    "    # Extract frames and landmarks for both videos\n",
    "    print(\"Processing reference video...\")\n",
    "    ref_frames, ref_landmarks = extract_landmarks(reference_video)\n",
    "    print(\"Processing user video...\")\n",
    "    user_frames, user_landmarks = extract_landmarks(user_video)\n",
    "\n",
    "    # Make sure both videos have the same number of frames\n",
    "    num_frames = min(len(ref_frames), len(user_frames))\n",
    "    width = max(ref_frames[0].shape[1], user_frames[0].shape[1])\n",
    "    height = max(ref_frames[0].shape[0], user_frames[0].shape[0])\n",
    "\n",
    "    # Create VideoWriter for output\n",
    "    fps = 30  # adjust to your video FPS if needed\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width*2, height))\n",
    "\n",
    "    print(\"Creating comparison video...\")\n",
    "    for i in range(num_frames):\n",
    "        ref_frame = ref_frames[i].copy()\n",
    "        user_frame = user_frames[i].copy()\n",
    "\n",
    "        # Draw landmarks\n",
    "        ref_frame = draw_landmarks_on_frame(ref_frame, ref_landmarks[i], mp_drawing, mp_pose)\n",
    "        user_frame = draw_landmarks_on_frame(user_frame, user_landmarks[i], mp_drawing, mp_pose)\n",
    "\n",
    "        # Resize frames to same height if necessary\n",
    "        ref_frame = cv2.resize(ref_frame, (width, height))\n",
    "        user_frame = cv2.resize(user_frame, (width, height))\n",
    "\n",
    "        # Concatenate side by side\n",
    "        combined_frame = np.hstack((ref_frame, user_frame))\n",
    "\n",
    "        # Write to output video\n",
    "        out.write(combined_frame)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"Comparison video saved to {output_video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e9433b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting landmarks...\n",
      "Detected circle center: (357, 20), radius: 277\n",
      "Press 's' to select the release frame while watching the video.\n",
      "Selected release frame: 52\n",
      "Launch angle at release: 10.70 degrees\n",
      "Torso rotation speed at release: 135.45 deg/sec\n",
      "Annotated video saved to annotated_throw_skeleton.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Helper functions\n",
    "# -----------------------------\n",
    "def extract_landmarks(video_path):\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(static_image_mode=False,\n",
    "                        model_complexity=2,\n",
    "                        enable_segmentation=False,\n",
    "                        min_detection_confidence=0.5,\n",
    "                        min_tracking_confidence=0.5)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    landmarks_seq = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(image_rgb)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            landmarks_seq.append(results.pose_landmarks)\n",
    "        else:\n",
    "            landmarks_seq.append(None)\n",
    "\n",
    "    cap.release()\n",
    "    pose.close()\n",
    "    return frames, landmarks_seq\n",
    "\n",
    "def launch_angle_vector(landmarks, frame_idx, shoulder_side='RIGHT', image_shape=None):\n",
    "    mp_pose = mp.solutions.pose\n",
    "    lm = landmarks[frame_idx]\n",
    "    if lm is None:\n",
    "        return None, None, None\n",
    "\n",
    "    if shoulder_side=='RIGHT':\n",
    "        shoulder = lm.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
    "        elbow = lm.landmark[mp_pose.PoseLandmark.RIGHT_ELBOW.value]\n",
    "        wrist = lm.landmark[mp_pose.PoseLandmark.RIGHT_WRIST.value]\n",
    "    else:\n",
    "        shoulder = lm.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "        elbow = lm.landmark[mp_pose.PoseLandmark.LEFT_ELBOW.value]\n",
    "        wrist = lm.landmark[mp_pose.PoseLandmark.LEFT_WRIST.value]\n",
    "\n",
    "    dx = wrist.x - elbow.x\n",
    "    dy = elbow.y - wrist.y  # flip y for image coordinates\n",
    "    angle_rad = np.arctan2(dy, dx)\n",
    "    angle_deg = np.degrees(angle_rad)\n",
    "\n",
    "    elbow_px = wrist_px = shoulder_px = None\n",
    "    if image_shape:\n",
    "        h, w, _ = image_shape\n",
    "        elbow_px = (int(elbow.x*w), int(elbow.y*h))\n",
    "        wrist_px = (int(wrist.x*w), int(wrist.y*h))\n",
    "        shoulder_px = (int(shoulder.x*w), int(shoulder.y*h))\n",
    "\n",
    "    return angle_deg, elbow_px, wrist_px\n",
    "\n",
    "def torso_rotation_speed(landmarks_seq, fps):\n",
    "    speeds = []\n",
    "    for i in range(1, len(landmarks_seq)):\n",
    "        lm_prev = landmarks_seq[i-1]\n",
    "        lm_curr = landmarks_seq[i]\n",
    "        if lm_prev is None or lm_curr is None:\n",
    "            speeds.append(0)\n",
    "            continue\n",
    "        left_prev = lm_prev.landmark[mp.solutions.pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "        right_prev = lm_prev.landmark[mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
    "        left_curr = lm_curr.landmark[mp.solutions.pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "        right_curr = lm_curr.landmark[mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
    "\n",
    "        angle_prev = np.degrees(np.arctan2(left_prev.y - right_prev.y, left_prev.x - right_prev.x))\n",
    "        angle_curr = np.degrees(np.arctan2(left_curr.y - right_curr.y, left_curr.x - right_curr.x))\n",
    "        speed = (angle_curr - angle_prev) * fps\n",
    "        speeds.append(speed)\n",
    "    return speeds\n",
    "\n",
    "\n",
    "def detect_throw_circle(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (9,9), 2)\n",
    "\n",
    "    circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, dp=1.2, minDist=100,\n",
    "                               param1=50, param2=30, minRadius=50, maxRadius=500)\n",
    "    if circles is not None:\n",
    "        circles = np.uint16(np.around(circles))\n",
    "        x, y, r = circles[0][0]\n",
    "        return (x, y), r\n",
    "    else:\n",
    "        h, w = frame.shape[:2]\n",
    "        return (w//2, h//2), min(w,h)//3\n",
    "\n",
    "# -----------------------------\n",
    "# Main script\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = \"Male_final_discus.mp4\"  # replace with your video path\n",
    "    shoulder_side = 'RIGHT'        # change to 'LEFT' if left-handed throw\n",
    "    output_video_path = \"annotated_throw_skeleton.mp4\"\n",
    "\n",
    "    print(\"Extracting landmarks...\")\n",
    "    frames, landmarks_seq = extract_landmarks(video_path)\n",
    "    num_frames = len(frames)\n",
    "    h, w, _ = frames[0].shape\n",
    "\n",
    "    # Detect throwing circle\n",
    "    circle_center, circle_radius = detect_throw_circle(frames[0])\n",
    "    print(f\"Detected circle center: {circle_center}, radius: {circle_radius}\")\n",
    "\n",
    "    # Let user select release frame\n",
    "    print(\"Press 's' to select the release frame while watching the video.\")\n",
    "    release_frame = None\n",
    "    for idx, frame in enumerate(frames):\n",
    "        display_frame = frame.copy()\n",
    "        cv2.putText(display_frame, f\"Frame {idx}\", (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "        cv2.imshow(\"Select Release Frame\", display_frame)\n",
    "        key = cv2.waitKey(30) & 0xFF\n",
    "        if key == ord('s'):\n",
    "            release_frame = idx\n",
    "            break\n",
    "        elif key == ord('q'):\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    if release_frame is None:\n",
    "        print(\"No release frame selected. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # Launch angle\n",
    "    angle_deg, elbow_px, wrist_px = launch_angle_vector(landmarks_seq, release_frame, shoulder_side, frames[0].shape)\n",
    "    print(f\"Selected release frame: {release_frame}\")\n",
    "    print(f\"Launch angle at release: {angle_deg:.2f} degrees\")\n",
    "\n",
    "    # Torso rotation speed\n",
    "    fps = 30\n",
    "    rotation_speeds = torso_rotation_speed(landmarks_seq, fps)\n",
    "    rotation_at_release = rotation_speeds[release_frame-1] if release_frame > 0 else 0\n",
    "    print(f\"Torso rotation speed at release: {rotation_at_release:.2f} deg/sec\")\n",
    "\n",
    "    # MediaPipe drawing utils\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_pose = mp.solutions.pose\n",
    "\n",
    "    # Create short clip around release\n",
    "    clip_range = 15\n",
    "    start = max(release_frame - clip_range, 0)\n",
    "    end = min(release_frame + clip_range, num_frames)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, 30, (w, h))\n",
    "\n",
    "    for i in range(start, end):\n",
    "        frame = frames[i].copy()\n",
    "\n",
    "        # Draw skeleton\n",
    "        if landmarks_seq[i]:\n",
    "            mp_drawing.draw_landmarks(frame, landmarks_seq[i], mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "\n",
    "        # Draw feet positions\n",
    "        lm = landmarks_seq[i]\n",
    "        if lm:\n",
    "            lf = lm.landmark[mp.solutions.pose.PoseLandmark.LEFT_FOOT_INDEX.value]\n",
    "            rf = lm.landmark[mp.solutions.pose.PoseLandmark.RIGHT_FOOT_INDEX.value]\n",
    "            lf_px = (int(lf.x*w), int(lf.y*h))\n",
    "            rf_px = (int(rf.x*w), int(rf.y*h))\n",
    "            color =  (0,0,255)\n",
    "            cv2.circle(frame, lf_px, 5, color, -1)\n",
    "            cv2.circle(frame, rf_px, 5, color, -1)\n",
    "\n",
    "        # Draw launch angle arrow at release\n",
    "        if i == release_frame and elbow_px and wrist_px:\n",
    "            cv2.arrowedLine(frame, elbow_px, wrist_px, (0,0,255), 4, tipLength=0.2)\n",
    "            cv2.putText(frame, f\"{angle_deg:.1f} deg\", (elbow_px[0], elbow_px[1]-20),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            cv2.putText(frame, f\"Rot speed: {rotation_at_release:.1f} deg/s\", (10,60),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,0,0), 2)\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"Annotated video saved to {output_video_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
